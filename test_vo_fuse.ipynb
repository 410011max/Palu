{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from models.palu_attention import HeadwiseLowRankModule\n",
    "from transformers.models.llama.modeling_llama import LlamaConfig\n",
    "\n",
    "config = LlamaConfig()\n",
    "\n",
    "q_len = 3\n",
    "group_size = 4\n",
    "num_heads = config.num_attention_heads\n",
    "hidden_dim = config.hidden_size\n",
    "total_rank_v = 4096\n",
    "out_features = config.hidden_size\n",
    "num_groups = num_heads // group_size\n",
    "head_dim = hidden_dim // num_heads\n",
    "group_rank = total_rank_v // num_groups\n",
    "group_dim = hidden_dim // num_groups\n",
    "\n",
    "v_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "o_proj = nn.Linear(hidden_dim, out_features, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_weight size: torch.Size([1, 32, 3, 3])\n",
      "v_states size: torch.Size([1, 32, 3, 128])\n",
      "attn_output size: torch.Size([1, 3, 4096])\n",
      "ori_output size: torch.Size([1, 3, 4096])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn(1, q_len, hidden_dim)\n",
    "# attn_weights: (bsz, num_heads, q_len, kv_seq_len)\n",
    "attn_weight = torch.randn(1, num_heads, q_len, q_len)\n",
    "# value_states: (bsz, q_len, num_key_value_heads, head_dim).transpose(1, 2)\n",
    "v_states = v_proj(inputs).view(1, q_len, num_heads, head_dim).transpose(1, 2)\n",
    "attn_output = torch.matmul(attn_weight, v_states)\n",
    "attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "attn_output = attn_output.reshape(1, q_len, -1)\n",
    "ori_output = o_proj(attn_output)\n",
    "\n",
    "print(f'attn_weight size: {attn_weight.size()}')\n",
    "print(f'v_states size: {v_states.size()}')\n",
    "print(f'attn_output size: {attn_output.size()}')\n",
    "print(f'ori_output size: {ori_output.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_weight size: torch.Size([1, 32, 3, 3])\n",
      "v_h_states size: torch.Size([1, 3, 4096])\n",
      "v_states size: torch.Size([1, 32, 3, 128])\n",
      "attn_output size: torch.Size([1, 32, 3, 128])\n"
     ]
    }
   ],
   "source": [
    "# Decompose and fuse v_proj and o_proj\n",
    "total_rank_v_list = [group_rank for _ in range(num_groups)]\n",
    "new_v_proj = HeadwiseLowRankModule.from_linear(v_proj, total_rank_v_list)\n",
    "\n",
    "# attn_weights: (bsz, num_groups, q_len * group_size, kv_seq_len)\n",
    "attn_weight = attn_weight.reshape(1, num_heads, q_len, q_len)\n",
    "print(f'attn_weight size: {attn_weight.size()}')\n",
    "\n",
    "# value_h_states: (bsz, q_len, num_groups, fused_group_dim).transpose(1, 2)\n",
    "v_h_states = new_v_proj.project_to_latent(inputs) #.reshape(1, q_len, num_groups, group_size).transpose(1, 2)\n",
    "print(f'v_h_states size: {v_h_states.size()}')\n",
    "\n",
    "# value_states: (bsz, q_len, num_key_value_heads, head_dim).transpose(1, 2)\n",
    "v_states = new_v_proj.reconstruct(v_h_states).reshape(1, q_len, num_heads, head_dim).transpose(1, 2)\n",
    "print(f'v_states size: {v_states.size()}')\n",
    "\n",
    "attn_output = torch.matmul(attn_weight, v_states)\n",
    "print(f'attn_output size: {attn_output.size()}')\n",
    "attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "attn_output = attn_output.reshape(1, q_len, -1)\n",
    "new_v_output = o_proj(attn_output)\n",
    "torch.testing.assert_close(ori_output, new_v_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_weight size: torch.Size([1, 8, 12, 3])\n",
      "v_h_states size: torch.Size([1, 8, 3, 512])\n",
      "attn_h_output size: torch.Size([1, 8, 12, 512])\n",
      "attn_h_output size: torch.Size([1, 32, 3, 512])\n",
      "attn_output size: torch.Size([1, 3, 4096])\n"
     ]
    }
   ],
   "source": [
    "# Decompose and fuse v_proj and o_proj\n",
    "total_rank_v_list = [group_rank for _ in range(num_groups)]\n",
    "new_v_proj = HeadwiseLowRankModule.from_linear(v_proj, total_rank_v_list)\n",
    "\n",
    "# attn_weights: (bsz, num_groups, q_len * group_size, kv_seq_len)\n",
    "attn_weight = attn_weight.reshape(1, num_groups, q_len * group_size, q_len)\n",
    "print(f'attn_weight size: {attn_weight.size()}')\n",
    "# value_h_states: (bsz, kv_seq_len, num_groups, group_rank).transpose(1, 2)\n",
    "v_h_states = new_v_proj.project_to_latent(inputs).reshape(1, q_len, num_groups, group_rank).transpose(1, 2)\n",
    "print(f'v_h_states size: {v_h_states.size()}')\n",
    "\n",
    "# attn_h_output: (bsz, num_heads, q_len * group_size, group_rank)\n",
    "attn_h_output = torch.matmul(attn_weight, v_h_states)\n",
    "print(f'attn_h_output size: {attn_h_output.size()}')\n",
    "attn_h_output = attn_h_output.reshape(1, num_heads, q_len, group_rank)\n",
    "print(f'attn_h_output size: {attn_h_output.size()}')\n",
    "\n",
    "outputs = []\n",
    "total_dims = 0\n",
    "total_ranks = 0\n",
    "for i in range(num_heads):\n",
    "    output = F.linear(attn_h_output[:, i, :, :], \n",
    "                      new_v_proj.U[total_dims:total_dims + head_dim, total_ranks : total_ranks + group_rank])\n",
    "    outputs.append(output)\n",
    "    total_dims += head_dim\n",
    "    if total_dims == group_dim:\n",
    "        total_dims = 0\n",
    "        total_ranks += group_rank\n",
    "\n",
    "new_attn_output = torch.cat(outputs, dim=-1)\n",
    "print(f'attn_output size: {new_attn_output.size()}')\n",
    "torch.testing.assert_close(attn_output, new_attn_output)\n",
    "\n",
    "new_v_output = o_proj(new_attn_output)\n",
    "torch.testing.assert_close(ori_output, new_v_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_weight size: torch.Size([1, 8, 12, 3])\n",
      "v_h_states size: torch.Size([1, 8, 3, 512])\n",
      "attn_h_output size: torch.Size([1, 8, 12, 512])\n",
      "attn_h_output size: torch.Size([1, 32, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "# Decompose and fuse v_proj and o_proj\n",
    "total_rank_v_list = [group_rank for _ in range(num_groups)]\n",
    "new_v_proj = HeadwiseLowRankModule.from_linear(v_proj, total_rank_v_list)\n",
    "\n",
    "# attn_weights: (bsz, num_groups, q_len * group_size, kv_seq_len)\n",
    "attn_weight = attn_weight.reshape(1, num_groups, q_len * group_size, q_len)\n",
    "print(f'attn_weight size: {attn_weight.size()}')\n",
    "# value_h_states: (bsz, kv_seq_len, num_groups, group_rank).transpose(1, 2)\n",
    "v_h_states = new_v_proj.project_to_latent(inputs).reshape(1, q_len, num_groups, group_rank).transpose(1, 2)\n",
    "print(f'v_h_states size: {v_h_states.size()}')\n",
    "\n",
    "# attn_h_output: (bsz, num_heads, q_len * group_size, group_rank)\n",
    "attn_h_output = torch.matmul(attn_weight, v_h_states)\n",
    "print(f'attn_h_output size: {attn_h_output.size()}')\n",
    "attn_h_output = attn_h_output.reshape(1, num_heads, q_len, group_rank)\n",
    "print(f'attn_h_output size: {attn_h_output.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_o_proj size: torch.Size([4096, 16384])\n"
     ]
    }
   ],
   "source": [
    "fused_hidden_dim = group_rank * num_heads\n",
    "new_o_proj = nn.Linear(fused_hidden_dim, out_features, bias=False)\n",
    "new_o_weight = torch.zeros(new_o_proj.weight.size())\n",
    "\n",
    "total_dims = 0\n",
    "total_dims_2 = 0\n",
    "total_ranks = 0\n",
    "total_fused_dims = 0\n",
    "for _ in range(num_groups):\n",
    "    for _ in range(group_size):\n",
    "        new_o_weight[:, total_fused_dims:total_fused_dims + group_rank] = \\\n",
    "            o_proj.weight.data[:, total_dims_2:total_dims_2 + head_dim] @ \\\n",
    "            new_v_proj.U.data[total_dims:total_dims + head_dim, total_ranks : total_ranks + group_rank]\n",
    "\n",
    "        total_dims += head_dim\n",
    "        total_dims_2 += head_dim\n",
    "        total_fused_dims += group_rank\n",
    "\n",
    "    total_dims = 0\n",
    "    total_ranks += group_rank\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(f'new_o_proj size: {new_o_proj.weight.data.size()}')\n",
    "    new_o_proj.weight.copy_(new_o_weight)\n",
    "final_fused_o_output = new_o_proj(attn_h_output.transpose(1, 2).reshape(1, q_len, -1))\n",
    "torch.testing.assert_close(ori_output, final_fused_o_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "palu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
