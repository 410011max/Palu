{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from models.palu_attention import HeadwiseLowRankModule\n",
    "\n",
    "v_proj = nn.Linear(80, 80, bias=False)\n",
    "o_proj = nn.Linear(80, 80, bias=False)\n",
    "num_groups = 2\n",
    "group_size = 5\n",
    "num_heads = 10\n",
    "head_dim = 8\n",
    "hidden_dim = 80\n",
    "rank_v = 80\n",
    "group_rank = rank_v // num_groups\n",
    "group_dim = hidden_dim // num_groups\n",
    "q_len = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_weight size: torch.Size([1, 10, 1, 1])\n",
      "v_states size: torch.Size([1, 10, 1, 8])\n",
      "attn_output size: torch.Size([1, 1, 80])\n",
      "ori_output size: torch.Size([1, 1, 80])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn(1, q_len, 80)\n",
    "# attn_weights: (bsz, self.num_heads, q_len, kv_seq_len)\n",
    "attn_weight = torch.randn(1, num_heads, q_len, q_len)\n",
    "# value_states: (bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "v_states = v_proj(inputs).view(1, q_len, num_heads, head_dim).transpose(1, 2)\n",
    "attn_output = torch.matmul(attn_weight, v_states)\n",
    "attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "attn_output = attn_output.reshape(1, q_len, -1)\n",
    "ori_output = o_proj(attn_output)\n",
    "\n",
    "print(f'attn_weight size: {attn_weight.size()}')\n",
    "print(f'v_states size: {v_states.size()}')\n",
    "print(f'attn_output size: {attn_output.size()}')\n",
    "print(f'ori_output size: {ori_output.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_weight size: torch.Size([1, 10, 1, 1])\n",
      "v_states size: torch.Size([1, 10, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "# Decompose and fuse v_proj and o_proj\n",
    "rank_v_list = [group_rank for _ in range(num_groups)]\n",
    "new_v_proj = HeadwiseLowRankModule.from_linear(v_proj, rank_v_list)\n",
    "\n",
    "# attn_weights: (bsz, self.num_groups, q_len * self.group_size, kv_seq_len)\n",
    "attn_weight = attn_weight.reshape(1, num_heads, q_len, q_len)\n",
    "print(f'attn_weight size: {attn_weight.size()}')\n",
    "# value_states: (bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "v_states = v_proj(inputs).view(1, q_len, num_heads, head_dim).transpose(1, 2)\n",
    "print(f'v_states size: {v_states.size()}')\n",
    "\n",
    "attn_output = torch.matmul(attn_weight, v_states)\n",
    "attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "attn_output = attn_output.reshape(1, q_len, -1)\n",
    "new_v_output = o_proj(attn_output)\n",
    "torch.testing.assert_close(ori_output, new_v_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_weight size: torch.Size([1, 10, 1, 1])\n",
      "v_h_states size: torch.Size([1, 1, 80])\n",
      "v_states size: torch.Size([1, 10, 1, 8])\n",
      "attn_output size: torch.Size([1, 10, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "# Decompose and fuse v_proj and o_proj\n",
    "rank_v_list = [group_rank for _ in range(num_groups)]\n",
    "new_v_proj = HeadwiseLowRankModule.from_linear(v_proj, rank_v_list)\n",
    "\n",
    "# attn_weights: (bsz, self.num_groups, q_len * self.group_size, kv_seq_len)\n",
    "attn_weight = attn_weight.reshape(1, num_heads, q_len, q_len)\n",
    "print(f'attn_weight size: {attn_weight.size()}')\n",
    "\n",
    "# value_h_states: (bsz, q_len, self.num_groups, self.fused_group_dim).transpose(1, 2)\n",
    "v_h_states = new_v_proj.project_to_latent(inputs) #.reshape(1, q_len, num_groups, group_size).transpose(1, 2)\n",
    "print(f'v_h_states size: {v_h_states.size()}')\n",
    "\n",
    "# value_states: (bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "v_states = new_v_proj.reconstruct(v_h_states).reshape(1, q_len, num_heads, head_dim).transpose(1, 2)\n",
    "print(f'v_states size: {v_states.size()}')\n",
    "\n",
    "attn_output = torch.matmul(attn_weight, v_states)\n",
    "print(f'attn_output size: {attn_output.size()}')\n",
    "attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "attn_output = attn_output.reshape(1, q_len, -1)\n",
    "new_v_output = o_proj(attn_output)\n",
    "torch.testing.assert_close(ori_output, new_v_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_weight size: torch.Size([1, 2, 5, 1])\n",
      "v_h_states size: torch.Size([1, 2, 1, 40])\n",
      "attn_h_output size: torch.Size([1, 10, 1, 40])\n",
      "attn_output size: torch.Size([1, 1, 80])\n"
     ]
    }
   ],
   "source": [
    "# Decompose and fuse v_proj and o_proj\n",
    "rank_v_list = [group_rank for _ in range(num_groups)]\n",
    "new_v_proj = HeadwiseLowRankModule.from_linear(v_proj, rank_v_list)\n",
    "\n",
    "# attn_weights: (bsz, self.num_groups, q_len * self.group_size, kv_seq_len)\n",
    "attn_weight = attn_weight.reshape(1, num_groups, q_len * group_size, q_len)\n",
    "print(f'attn_weight size: {attn_weight.size()}')\n",
    "# value_h_states: (bsz, kv_seq_len, self.num_groups, self.fused_group_dim).transpose(1, 2)\n",
    "v_h_states = new_v_proj.project_to_latent(inputs).reshape(1, q_len, num_groups, group_rank).transpose(1, 2)\n",
    "print(f'v_h_states size: {v_h_states.size()}')\n",
    "\n",
    "attn_h_output = torch.matmul(attn_weight, v_h_states).reshape(1, num_heads, q_len, group_rank)\n",
    "print(f'attn_h_output size: {attn_h_output.size()}')\n",
    "\n",
    "outputs = []\n",
    "total_dims = 0\n",
    "total_ranks = 0\n",
    "for i in range(num_heads):\n",
    "    output = F.linear(attn_h_output[:, i:i+1, :, :], \n",
    "                      new_v_proj.U[total_dims:total_dims + head_dim, total_ranks : total_ranks + group_rank])\n",
    "    outputs.append(output)\n",
    "    total_dims += head_dim\n",
    "    if total_dims == group_dim:\n",
    "        total_dims = 0\n",
    "        total_ranks += group_rank\n",
    "\n",
    "new_attn_output = torch.cat(outputs, dim=-1).reshape(1, q_len, -1)\n",
    "print(f'attn_output size: {new_attn_output.size()}')\n",
    "\n",
    "\n",
    "# # 重新排列 new_v_proj.U，使其可以一次性計算所有的頭部\n",
    "# reshaped_U = new_v_proj.U.view(num_heads, head_dim, -1).transpose(1, 2)\n",
    "\n",
    "# # 使用 torch.matmul 進行矩陣乘法\n",
    "# outputs = torch.matmul(attn_h_output, reshaped_U)\n",
    "\n",
    "# # 重新排列 outputs，使其回到原來的形狀\n",
    "# new_attn_output = outputs.view(1, q_len, -1)\n",
    "\n",
    "\n",
    "torch.testing.assert_close(attn_output, new_attn_output)\n",
    "\n",
    "attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "attn_output = attn_output.reshape(1, q_len, -1)\n",
    "new_v_output = o_proj(attn_output)\n",
    "torch.testing.assert_close(ori_output, new_v_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose and fuse v_proj and o_proj\n",
    "rank_v_list = [group_rank for _ in range(num_groups)]\n",
    "new_v_proj = HeadwiseLowRankModule.from_linear(v_proj, rank_v_list)\n",
    "\n",
    "# attn_weights: (bsz, self.num_groups, q_len * self.group_size, kv_seq_len)\n",
    "attn_weight = attn_weight.reshape(1, num_groups, q_len * group_size, q_len)\n",
    "print(f'attn_weight size: {attn_weight.size()}')\n",
    "# value_h_states: (bsz, kv_seq_len, self.num_groups, self.fused_group_dim).transpose(1, 2)\n",
    "v_h_states = new_v_proj.project_to_latent(inputs).reshape(1, q_len, num_groups, group_rank).transpose(1, 2)\n",
    "print(f'v_h_states size: {v_h_states.size()}')\n",
    "\n",
    "attn_h_output = torch.matmul(attn_weight, v_h_states).reshape(1, num_heads, q_len, group_rank)\n",
    "print(f'attn_h_output size: {attn_h_output.size()}')\n",
    "\n",
    "# outputs = []\n",
    "# total_dims = 0\n",
    "# total_ranks = 0\n",
    "# for i in range(num_heads):\n",
    "#     output = F.linear(attn_h_output[:, i:i+1, :, :], \n",
    "#                       new_v_proj.U[total_dims:total_dims + head_dim, total_ranks : total_ranks + group_rank])\n",
    "#     outputs.append(output)\n",
    "#     total_dims += head_dim\n",
    "#     if total_dims == group_dim:\n",
    "#         total_dims = 0\n",
    "#         total_ranks += group_rank\n",
    "\n",
    "# new_attn_output = torch.cat(outputs, dim=-1).reshape(1, q_len, -1)\n",
    "# print(f'attn_output size: {new_attn_output.size()}')\n",
    "\n",
    "\n",
    "# 重新排列 new_v_proj.U，使其可以一次性計算所有的頭部\n",
    "reshaped_U = new_v_proj.U.view(num_heads, head_dim, -1).transpose(1, 2)\n",
    "\n",
    "# 使用 torch.matmul 進行矩陣乘法\n",
    "outputs = torch.matmul(attn_h_output, reshaped_U)\n",
    "\n",
    "# 重新排列 outputs，使其回到原來的形狀\n",
    "outputs = outputs.view(1, q_len, -1)\n",
    "\n",
    "\n",
    "torch.testing.assert_close(attn_output, new_attn_output)\n",
    "\n",
    "attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "attn_output = attn_output.reshape(1, q_len, -1)\n",
    "new_v_output = o_proj(attn_output)\n",
    "torch.testing.assert_close(ori_output, new_v_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "palu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
